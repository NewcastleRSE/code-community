{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2QYgfqvaAiN"
      },
      "source": [
        "# Image classification\n",
        "## Objectives\n",
        "- To learn how a convolutional neural network works\n",
        "- To train a few image classification models using a web interface (Teachable Machine)\n",
        "- To put those models to use in a python script\n",
        "\n",
        "## Introduction\n",
        "### The problem\n",
        "Imagine the following scenario: you are trying to figure out whether there is a relationship between the medium in which an artwork is created, and its subject matter. To do this, you need to see artworks -- lots of artworks. So you go to a few museums, but quickly realise you need a more systematic way of gathering your data. Plus, as is well known, the majority of a museum's collection is hidden away in the archives, not on display in the galleries. So you ask the museums whether they can give you digital facsimiles of their collections -- by the end of it, you end up with over a million images in your hard drive. What do you do?\n",
        "### The solution\n",
        "Sure, you could go through each of the images one by one and classify them as being sculptures, or paintings, or drawings, and classify their subject matter as landscape, abstract, or portrait. You would get a high-quality dataset at the end of the process, but it would take you months to get there. You don't have months. You have two days.\n",
        "\n",
        "This is where a computational approach can help you. Specifically, a subfield of Artificial Intelligence known as *computer vision,* which essentially allows automated systems to interpret the visual world in a human-like way. It is put to use in cutting-edge scenarios like self-driving cars -- to help distinguish between pedestrians, obstacles, traffic lights, for example. If you have driven a car made in the last four or five years, you might have seen examples of this: when the dashboard shows you the speed limit for the road you are currently on is a real-world application of computer vision: a camera at the front of the car noticed and identified a speed limit sign. More nefarious uses of computer vision also exist in the wild: [famously China uses security cameras to track the movements of its citizens](https://www.npr.org/2021/01/05/953515627/facial-recognition-and-beyond-journalist-ventures-inside-chinas-surveillance-sta?t=1652430994420).\n",
        "\n",
        "![Automatic traffic sign detection](imgs\\speed_limiter_header.jpg)\n",
        "\n",
        "Our use case is, however, more innocent (though still problematic) and much simpler than the complicated systems in place for state surveillance or self-driving cars. All we need is an image-level classification to identify a work's medium and subject matter. To do that, we need to teach a machine how to identify these two characteristics and we will do that using a point-and-click webservice that is aptly called [Teachable Machine](https://teachablemachine.withgoogle.com/).\n",
        "\n",
        "## Teachable Machine, Machine Learning, and Convolutional Neural Networks\n",
        "Google's [Teachable Machine](https://teachablemachine.withgoogle.com/) is a web app that allows users to easily train and test simple image and sound classification models. The image classification models refine the pre-trained, popular [MobileNet](https://arxiv.org/abs/1704.04861) models, a small, low-latency and low-powered model designed to be used by mobile devices. By *model* here, we mean a *machine learning* model.\n",
        "\n",
        "### Machine Learning\n",
        "*Machine learning* is a subfield of AI dedicated to allow computer systems to learn about a dataset and, then, make predictions *based on that learning.* For example, if we have access to all the past data relating to loan applications, we can use that to *predict* whether a particular applicant is likely to be granted or refused a loan.\n",
        "\n",
        "Essentially, *machine learning* as a field of study tries to find the best ways in which computer systems can learn about the data. There are many different machine learning methods, but these can be roughly divided into two different groups: **supervised** and **unsupervised** machine learning.\n",
        "\n",
        "- **supervised learning** means that the computer will learn from a dataset that has been, in some way, curated by a human -- usually for classification tasks in which we present examples of each category.\n",
        "\n",
        "- **unsurpervised learning** means that we won't tell the computer anything about the dataset -- usually for clustering or generative tasks.\n",
        "\n",
        "MobileNet, used by Teachable Machine, is a specific type of supervised learning model known as a *Convolutional Neural Network.*\n",
        "\n",
        "#### Convolutional Neural Networks\n",
        "[Convolutional Neural Networks (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) are a type of algorithms inspired by brain architecture (hence the 'Neural'), and are commonly used to analyse visual imagery. The details are too complex to get into here, but essentially, you can imagine a series of 'neurons' arranged in layers; each layer of neurons is responsible for identifying a particular characteristic of the input image in increasing complexity -- for example, first edges, then surfaces, then what those surfaces are (the feature extraction phase), then putting it all together, then identifying what the object is (the classification phase).\n",
        "\n",
        "![Convolutional Neural Network Diagram](imgs\\convolutional_neural_network.png)\n",
        "\n",
        "When we fine-tune a CNN, what we do, essentially, is modify its last layer, i.e., the classification, so that the predicted results fall into the categories that we want.\n",
        "\n",
        "### Teachable Machine walkthrough\n",
        "Because Teachable Machine is a web-app, we can train our simple model easily through the browser, without having to worry too much about how it works under the hood (right now). So how do we do this?\n",
        "\n",
        "1. Head over to [Teachable Machine](https://teachablemachine.withgoogle.com/)\n",
        "2. Click 'Get Started'\n",
        "3. Select 'Image Project'\n",
        "4. Select 'Standard Image Project'\n",
        "\n",
        "You should now see a simple diagrammatic view of what you need to do next: on the left, two windows entitled 'Class 1' and 'Class 2,' plus options to use the webcam or upload files; in the middle, a window enticengly entitled 'Training,' and at the end, 'Preview.'\n",
        "\n",
        "![Techable Machine interface](imgs\\TM_interface.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qToa8YML1-tX"
      },
      "source": [
        "## Try the interface\n",
        "\n",
        "We'll begin by trying the Teachable Machine interface in a dataset unrelated to our problem, the [Caltech101](https://en.wikipedia.org/wiki/Caltech_101).\n",
        "\n",
        "The complete dataset contains nearly 10000 images, split into 101 image-level categories (hence the 101 in the title of the dataset). Although this particular dataset is relatively small, 101 categories are a little much, and we won't need the full 10000 images to train our (very small) model, so we will use a subset of the data.\n",
        "\n",
        "### Test data\n",
        "\n",
        "You should see a folder called `caltech101_dataset`. Inside that folder, you should see five more folders: `airplanes`, `butterfly`, `Faces`, `Motorbikes`, and `sunflower` -- these are the categories we are going to be training our model for (5 of the 101 available in the entire dataset). Each of those folders has a number of random images from that category, have a look at a few to get a sense of their diversity (or lack thereof), quality, size, etc. You might already begin to see a few problems with the data, but right now we are only interested in it insofar as it will allow us to quickly learn how to train a model using Teachable Machine.\n",
        "\n",
        "### Upload the images to Teachable Machine\n",
        "\n",
        "1. Head over to [Teachable Machine](https://teachablemachine.withgoogle.com/)\n",
        "2. Click 'Get Started'\n",
        "3. Select 'Image Project'\n",
        "4. Select 'Standard Image Project'\n",
        "5. Click the 'Upload' button in 'Class 1'\n",
        "6. Select all the images inside the `airplanes` folder\n",
        "7. Rename 'Class 1' to 'Airplanes'\n",
        "8. Repeat the process (points 5-7) to the remaining categories\n",
        "\n",
        "At the end of this process, you should see something like this:\n",
        "![](imgs\\TM_Categories.png)\n",
        "\n",
        "You might be able to spot a few more drawbacks with this dataset. For example, there are a lot more samples of Airplanes or Motorbikes, than Sunflowers or Butterflies. We might see how this affects our models abilities to categorise an image later on.\n",
        "\n",
        "#### What happens when you train a model\n",
        "\n",
        "From here, we could just click the 'Train Model' button and be off to the races, but before we do that, there are a few parameters in the 'Advanced' tab that are worth looking through. These are:\n",
        "\n",
        "- `Epochs`\n",
        "- `Batch Size`\n",
        "- `Learning Rate`\n",
        "\n",
        "Before we explore what each of these terms means, it will be good to have a sense of what happens when we *train* or *fine-tune* a model. \n",
        "\n",
        "When we train a model, we are feeding the algorithm each image of our dataset; for each image, the algorithm will put it through the various layers we discussed above in order to examine their characteristics, and then attribute those characteristics to the class we categorise the image as -- the more images we feed the algorithm, the easier it will be for it to distinguish between essential characteristics (i.e., the ones that define an airplane as an airplane), and the non-essential characteristics (i.e., things that are part of the image that do not make an airplane).\n",
        "\n",
        "Another thing that happens during training is that the dataset is further divided into a `training set` and a `validating set`. This means that a certain percentage of the dataset is reserved to validate the results of the training. In Teachable Machine this is done automatically for you with an 80/20 split. Once we train the model, we can explore a little more what the point of this division is.\n",
        "\n",
        "With this very short and high-level explanation in mind, here's what those terms mean:\n",
        "\n",
        "- `Epochs` are the number of times the entire dataset is fed to the algorithm. Once all the images have been analysed and classified, we have one epoch. You would tweak this number to try to improve the predictive capabilities of your algorithm. (Though be careful of [*overfitting*](https://www.ibm.com/cloud/learn/overfitting))\n",
        "\n",
        "- `Batch Size` defines how many samples are fed to the algorithm at the same time; typically, you'd want to tweak this value if you are finding problems with system performance (i.e., it's taking too long to retrain the model).\n",
        "\n",
        "- `Learning Rate` is a parameter that -- metaphorically -- defines how quickly the model learns about the data. Small tweaks to this value will influence the predictive capabilities of the model. What `Learning Rate` actually *is* is a little more complex, you can start by [reading the Wiki article about it](https://en.wikipedia.org/wiki/Learning_rate).\n",
        "\n",
        "### Train the model\n",
        "\n",
        "Click the 'Train Model' button, and *do not change tabs or close the browser*. You will see a progress bar that will count through the `epochs` we've defined earlier. At the end of this process, the 'Preview' window will become active, and we will be able to test how capable our model is at sorting images into the categories we've defined above.\n",
        "\n",
        "### Test the model\n",
        "\n",
        "Now, finally, is time for the fun stuff. [Search for a few images for each category](https://images.google.com/) and see whether the model categorises them accurately. To test each image, make sure that the 'Input' radio button is active, and that 'File' is selected in the dropdown menu. Try to find images that would lead to a miscategorization, and try to understand why they would be miscategorised. How would you tweak the training to correct that? Finally, click on the 'Under the hood' button in the 'Training' window, and have a look at the statistical results of your training. Try to make sense of it, particularly the 'Confusion Matrix' which is one of the main tools in assessing a model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEkZQSfifOr9"
      },
      "source": [
        "## Do it all over again\n",
        "\n",
        "<div class = \"alert alert-block alert-info\">You can skip this section if all you are interested is putting the models to use: you should already have a `models` folder with a couple of pre-trained models. If you want to train the models yourself, <a href=\"https://drive.google.com/file/d/1YtRBLjBSzzFcuGVmiOznuwF1MoSolQnJ/view?usp=sharing\">all the training data is available in this .zip file</a></div>\n",
        "\n",
        "Now that we are relatively familiar with what *training* is, and we know how to work with Teachable Machine, it's time to train a couple of models that can actually be useful to our problem -- i.e., cataloguing a collection of artworks by medium and subject matter.\n",
        "\n",
        "A good model starts with good training data, and while we cannot guarantee that the data we're using is *good*, we can make sure it is adequate to our purposes. (And the only reason why we are not too concerned with the quality of our training set is because we are just doing this as a tutorial -- if we were to put this model to actual use, ensuring the quality of the training data is paramount).\n",
        "\n",
        "In the zip file you've downloaded above, there should be two other folders. One, `art_media_dataset`, will be used to train a model to identify the medium of the artwork; the other, `Wiki_Art_dataset` will be used to train a model capable of identifying the subject matter. The steps we will be taking are the same in both cases, and are the same as the ones we took for our caltech101 model above:\n",
        "\n",
        "1. Upload the contents of each folder inside the `art_media_dataset` to its own category\n",
        "2. Train the model (this will take a few minutes)\n",
        "3. Test the model in browser (with a random google image)\n",
        "4. Export the model to your machine:\n",
        "  1. Click the 'Export Model' button at the top of the 'Preview Window'\n",
        "  2. Select the 'Tensorflow' tab (*not* Tensorflow.js)\n",
        "  3. Make sure you have 'Keras' selected in the 'Model Conversion Type'\n",
        "  4. Click 'Download my model' and save it to someplace where you can find it (converting the model will take a few minutes). The result will be a zip file containing two files: one will be a `.h5` which is the model itself, the other is a text file with the labels for each category. Rename the zipfile so that you can distinguish between both models.\n",
        "\n",
        "Repeat the process for the `Wiki_Art_dataset`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upSG97PxM1bb"
      },
      "source": [
        "## Load the model\n",
        "\n",
        "Now it's time to load the model. First we will need a few libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install pip package in the current jupyter kernel\n",
        "import sys\n",
        "!{sys.executable} -m pip install keras\n",
        "!{sys.executable} -m pip install Pillow\n",
        "!{sys.executable} -m pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE-JinyaM8kf"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mabjp0bTNaiR"
      },
      "source": [
        "The `keras.models` library is the one which will actually allow you to read the `.h5` file we saved earlier.\n",
        "\n",
        "The `PIL` library is the standard python library for image manipulation, of which we will need to do a little to get a prediction from the model.\n",
        "\n",
        "Finally, `numpy` is another well-known library to work with scientific data. We will use this to create an input array for predictions.\n",
        "\n",
        "Now it's time to actually load the models into memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9DHQ-EcOCmj"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model_medium = load_model('models/converted_keras_medium/keras_model_medium.h5', compile=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is3PmyxeP8-S"
      },
      "source": [
        "## Load an image and test the model\n",
        "\n",
        "Now we will need to make sure the model is working, so we will test it with a single image.\n",
        "\n",
        "We'll start by loading an image into memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9BOlKGpPjM8"
      },
      "outputs": [],
      "source": [
        "image = Image.open('test/image3.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub5ZruHMP3P7"
      },
      "source": [
        "Now, we'll need to transform the image so that it is in the same format expected by the model. We will need to resize and crop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY6_oDN5QX4P"
      },
      "outputs": [],
      "source": [
        "size = (224, 224) # defines the size of the image to 224x224 px\n",
        "image = ImageOps.fit(image, size, Image.ANTIALIAS) # resizes the image to the size specified above and crops it into a square"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEiHuIKfQs7s"
      },
      "source": [
        "Next, we'll need to turn the image into a numpy array, which is the data needed by the model to make the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7QtL-6cQsWI"
      },
      "outputs": [],
      "source": [
        "image_array = np.asarray(image)\n",
        "normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SMSTQFTQ_by"
      },
      "source": [
        "We now need to create the input array, and insert our new transformed image into it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofgSfsuGRGe7"
      },
      "outputs": [],
      "source": [
        "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
        "data[0] = normalized_image_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQp1HYEtRUFG"
      },
      "source": [
        "Finally, we can get a prediction from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsSd0_FKRTuy",
        "outputId": "1cc54d35-7611-473b-bdbb-f90f54d01b69"
      },
      "outputs": [],
      "source": [
        "prediction_medium = model_medium.predict(data)\n",
        "print(prediction_medium)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZxnl_cbT3TK"
      },
      "source": [
        "This looks a little like gibberish, but it isn't. Remember that our model for medium had four classes ['Drawing', 'Engraving', 'Painting', 'Sculpture'] and what we have here are the values for each of those classes. What we need to do is find the highest value, and translate it to a class and confidence percentage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_43rySrKUkLw",
        "outputId": "ee5b80da-638b-420c-9c8b-1d127f4ec143"
      },
      "outputs": [],
      "source": [
        "category_medium = np.argmax(prediction_medium, axis = 1) # Returns an array with index of the maximum values\n",
        "confidence_medium = prediction_medium[0][category_medium] # Returns an array with the maximum confidence value \n",
        "print(f\"This is a {category_medium[0]} with confidence of {confidence_medium[0]}\") # Prints the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7f8DBabYkJc"
      },
      "source": [
        "This is a little better, but still not ideal. What we want is to have a completely readable result, rather than a category number. Luckily, we have that `labels.txt` that we can put to good use now. We need to open it and turn it into a dictionary, so that we can print the actual category, rather than its index. So let's do that now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2MIrLVuZCbo",
        "outputId": "479c714c-d399-4453-b338-64cfafd244ad"
      },
      "outputs": [],
      "source": [
        "labels_medium_dictionary = {}\n",
        "with open('models/converted_keras_medium/labels_medium.txt') as labels_medium: # opens the file in a context manager\n",
        "  for line in labels_medium:  # for each line in the file\n",
        "    (key, value) = line.split(\" \", 1) # splits the line into two (by whitespace); first value becomes the key, the second the value\n",
        "    labels_medium_dictionary[int(key)] = value.rstrip(\"\\n\") # creates the entry in the dictionary\n",
        "\n",
        "print(labels_medium_dictionary) # prints the result to the screen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMwJIRdBaIiD"
      },
      "source": [
        "Now that we have our dictionary, we can easily make our prediction much more readable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LZ31uFAaOaN",
        "outputId": "6228d646-474b-4381-e3b7-6678f32c09a8"
      },
      "outputs": [],
      "source": [
        "print(f\"This is a {labels_medium_dictionary[int(category_medium[0])]} with a confidence of {confidence_medium[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqvrzAr6gyli"
      },
      "source": [
        "We can now do the same for our genre model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmLbXQf4l-fR"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model_genre = load_model('models/converted_keras_genre/keras_model_genre.h5', compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE70XwtLg2iZ",
        "outputId": "30bb6b12-5874-4a30-8462-8e8a1f6ce810"
      },
      "outputs": [],
      "source": [
        "# Create labels dictionary\n",
        "labels_genre_dictionary = {}\n",
        "with open('models/converted_keras_genre/labels_genre.txt') as labels_genre: # opens the file in a context manager\n",
        "  for line in labels_genre:  # for each line in the file\n",
        "    (key, value) = line.split(\" \", 1) # splits the line into two (by whitespace); first value becomes the key, the second the value\n",
        "    labels_genre_dictionary[int(key)] = value.rstrip(\"\\n\") # creates the entry in the dictionary\n",
        "\n",
        "print(labels_genre_dictionary) # prints the result to the screen\n",
        "\n",
        "# Get prediction\n",
        "prediction_genre = model_genre.predict(data)\n",
        "print(prediction_genre)\n",
        "\n",
        "# Gets highest value prediction and confidence\n",
        "category_genre = np.argmax(prediction_medium, axis = 1) # Returns an array with index of the maximum values\n",
        "confidence_genre = prediction_medium[0][category_genre] # Returns an array with the maximum confidence value \n",
        "\n",
        "# Prints the result in readable form\n",
        "print(f\"This is a {labels_genre_dictionary[int(category_genre[0])]} with a confidence of {confidence_genre[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVOjwWYsjXvX"
      },
      "source": [
        "Now, to put all the results together is just a question of tailoring the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m18tUblpjhF_",
        "outputId": "b9938f12-4a55-4fb5-a6b6-f2a7453b3f21"
      },
      "outputs": [],
      "source": [
        "print(f\"Medium: {labels_medium_dictionary[int(category_medium[0])]}, Confidence: {confidence_medium[0]}\\n \\\n",
        "Genre: {labels_genre_dictionary[int(category_genre[0])]}, Confidence: {confidence_genre[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6UI0ZVDmIWh"
      },
      "source": [
        "## Doing it over and over again\n",
        "\n",
        "You might be thinking: why should I go through all this work just to check the results for one image, when we could do that just as easily through the web interface? That's a fair point, but remember, you're not doing all this for one image, you're doing it for a million! Luckily, writing a script to get classifications is just as easy to do for one as it is to do for one million. All we need is a little `for` loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nMjGA9koN3M"
      },
      "source": [
        "Before that, we just need to collect some information about the names and number of files we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWpNS7r-mvjX"
      },
      "outputs": [],
      "source": [
        "import os, os.path # imports the libraries to work with file paths\n",
        "\n",
        "# We create a constant with the directory where the images we want to test are\n",
        "IMAGE_DIR = \"data_to_label/\"\n",
        "\n",
        "# Get a list of file names\n",
        "file_names = [name for name in os.listdir(IMAGE_DIR) if '.jpg' in name]\n",
        "\n",
        "# Get total number of files\n",
        "nr_files = len(file_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MUF23dOos7-"
      },
      "source": [
        "Now that we now how many files we have, and what their names are, we can transform the images into a numpy array, like we did above. To make the code more readable, we begin by creating a little function that does that for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-cBEl2Ho6Lr"
      },
      "outputs": [],
      "source": [
        "def transform_image(image_path):\n",
        "  image = Image.open(image_path)\n",
        "  size = (224, 224) # defines the size of the image to 224x224 px\n",
        "  image = ImageOps.fit(image, size, Image.ANTIALIAS) # resizes the image to the size specified above and crops it into a square\n",
        "  image_array = np.asarray(image)\n",
        "  normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
        "  return normalized_image_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee9pEy8_pmSY"
      },
      "source": [
        "Now we can create our input array, much like we did before, with one small difference: the first value will no longer be 1 (for a single image), but the number of images we want to test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYqubNYGpwnR"
      },
      "outputs": [],
      "source": [
        "data = np.ndarray(shape=(nr_files, 224, 224, 3), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "droSnheMp7to"
      },
      "source": [
        "We can test we've done it correctly by printing how long the array is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVYqW-v7p__Q",
        "outputId": "978a1209-a5be-4869-bbce-dd4808780835"
      },
      "outputs": [],
      "source": [
        "print(len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3KYvfeJqCZQ"
      },
      "source": [
        "Now, the final step we need to take to create our input array is to iterate through our list of files and, for each one, normalise the image and add it to our data array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH6K8_VFqNtS"
      },
      "outputs": [],
      "source": [
        "for i, file in enumerate(file_names):\n",
        "  file_path = IMAGE_DIR + file\n",
        "  print(f\"[{i}]: {file_path}\") # just to check we are iterating correctly\n",
        "  data[i] = transform_image(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjQnLcYCrJbY"
      },
      "source": [
        "Now, if we try to predict using this data array, we will see a slightly different result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGFj0fv2rX7E",
        "outputId": "258e8d2d-839c-4e15-bfe9-4823cca8a526"
      },
      "outputs": [],
      "source": [
        "prediction_medium = model_medium.predict(data)\n",
        "prediction_genre = model_genre.predict(data)\n",
        "print(prediction_medium)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYbKo9e0rnP0"
      },
      "source": [
        "Instead of having a single line with four values, we have multiple lines with four values each. You might have guessed it: each line corresponds to the predicted results for each image. So we now have to change our display code slightly, so that we are able to understand it. We iterate through our input files again, and print to console to make sure everything is readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00Qvk-BwsXBM"
      },
      "outputs": [],
      "source": [
        "category_medium = np.argmax(prediction_medium, axis = 1) # Returns an array with index of the maximum values for each file\n",
        "category_genre = np.argmax(prediction_genre, axis = 1) # Returns an array with index of the maximum values for each file\n",
        "\n",
        "for i, file in enumerate(file_names):\n",
        "  medium = labels_medium_dictionary[int(category_medium[i])]\n",
        "  confidence_medium = prediction_medium[i][category_medium[i]]*100 #to get a percentage value\n",
        "\n",
        "  genre = labels_genre_dictionary[int(category_genre[i])]\n",
        "  confidence_genre = prediction_genre[i][category_genre[i]]*100\n",
        "  print(f\"{file} -- Medium: {medium}({confidence_medium:.2f}%), Genre: {genre}({confidence_genre:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YIY2LywONVU"
      },
      "source": [
        "## Saving the output to a file\n",
        "\n",
        "Finally, the downside of this type of output is that it becomes very hard to read the results on the screen. It is much better if we could save it somewhere, and then read it later for analysis. That's exactly what we are doing next, simply saving the results to a `.csv` file, or 'comma-separated value' -- which is no more than a simple table in text form.\n",
        "\n",
        "We begin by creating the file in `append` mode, meaning, we can add stuff to it, rather than write over it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5upygUuO1mV"
      },
      "outputs": [],
      "source": [
        "with open('output.csv', 'a') as outfile:\n",
        "  outfile.write('file_name,medium,medium_confidence,genre,genre_confidence\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ogcun8PR8J"
      },
      "source": [
        "Now, we simply change the `for` loop above to write to the file, instead of writing to the screeen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZQvyXOGPbOA"
      },
      "outputs": [],
      "source": [
        "for i, file in enumerate(file_names):\n",
        "  medium = labels_medium_dictionary[int(category_medium[i])]\n",
        "  confidence_medium = prediction_medium[i][category_medium[i]]*100 #to get a percentage value\n",
        "\n",
        "  genre = labels_genre_dictionary[int(category_genre[i])]\n",
        "  confidence_genre = prediction_genre[i][category_genre[i]]*100\n",
        "  with open('output.csv', 'a') as outfile:\n",
        "    outfile.write(f\"{file},{medium},{confidence_medium:.2f},{genre},{confidence_genre:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9DVn0vRRThM"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "If you spend anytime combing through the results we got above, you will probably conclude that they are not very good -- in fact, you are right, they are terrible. There's an old dictum that states that:\n",
        "\n",
        "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Data Science is 99% preparation, 1% misinterpretation.</p>&mdash; Big Data Borat (@BigDataBorat) <a href=\"https://twitter.com/BigDataBorat/status/324892846685564930?ref_src=twsrc%5Etfw\">April 18, 2013</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
        "\n",
        "and you can see both of those at play in the results we have here. You can improve the results by doing a few things:\n",
        "\n",
        "### Improve the training data\n",
        "\n",
        "There's no fine point to it: the data we used to train both models is, at best, of dubious quality, and it simply isn't a very extensive training set. In both cases, we used a subset of a larger dataset which was, itself, of uncertain origin. With machine learning, if you put garbage in, you will get garbage out. This is the first point that needs to be addressed if you tried to improve the results of the models, don't even think about reading the other strategies below until you've addressed that.\n",
        "\n",
        "### Refine the categories\n",
        "\n",
        "Again, the way in which the dataset was divided is questionable at best. That kind of classification is best left to the domain experts -- which we are not. You can refine the categories to be more exclusive than they are now and then, see the point above, improve the training data to reflect that.\n",
        "\n",
        "### Tweak the parameters: `epochs`, `learning rate`\n",
        "Tweaking those parameters will give you the most immediate improvements (or deteriorations), but they will not fix the problems with your training data. A lot of trial and error, testing, and interpretation of results is involved in this tweaking, so be patient.\n",
        "\n",
        "While the results may not be great, the method to get to them is sound. For this type of application, you've learned to:\n",
        "\n",
        "1. Gather the data\n",
        "1. Organise the data\n",
        "1. Train the model\n",
        "1. Get predictions from the model\n",
        "\n",
        "Where this will take you next, is your call!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IZ_Fr59UY_6"
      },
      "source": [
        "## Acknowledgments and data sources\n",
        "\n",
        "### Acknowledgments\n",
        "This tutorial is greatly indebted to a (as yet unpublished) [Programming Historian](https://programminghistorian.org/) lesson, by [Nabeel Siddiqui](https://nabeelsiddiqui.net/). You can see the pre-publication draft [here](https://github.com/programminghistorian/ph-submissions/issues/414).\n",
        "\n",
        "### Data sources\n",
        "\n",
        "#### Caltech101\n",
        "A subset of this dataset was aquired through manipulation of the version [available here](https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/datasets.html#caltech-101).\n",
        "\n",
        "#### Art Images: Drawing/Painting/Sculptures/Engravings\n",
        "\n",
        "A subset of this dataset was aquired through manipulation of the version [available here](https://www.kaggle.com/datasets/thedownhill/art-images-drawings-painting-sculpture-engraving).\n",
        "\n",
        "#### WikiArt\n",
        "\n",
        "A subset of this dataset was aquired through manipulation of the version [available here](https://www.kaggle.com/datasets/ipythonx/wikiart-gangogh-creating-art-gan).\n",
        "\n",
        "#### Best Artworks of All Time\n",
        "\n",
        "A subset of this dataset was aquired through manipulation of the version [available here](https://www.kaggle.com/datasets/ikarus777/best-artworks-of-all-time)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ece44715f4212943b197e4adf644bf0e78785a6e30da3c23ea9919f5a3edfc79"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
